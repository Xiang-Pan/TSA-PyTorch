{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from pytorch_transformers import BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def insert(original, new, pos):\n",
    "# '''Inserts new inside original at pos.'''\n",
    "    return original[:pos] + new + original[pos:]\n",
    "\n",
    "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('loading tokenizer:', dat_fname)\n",
    "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        text = ''\n",
    "        for fname in fnames:\n",
    "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "            lines = fin.readlines()\n",
    "            fin.close()\n",
    "            for i in range(0, len(lines), 3):\n",
    "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "                aspect = lines[i + 1].lower().strip()\n",
    "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
    "                text += text_raw + \" \"\n",
    "\n",
    "        tokenizer = Tokenizer(max_seq_len)\n",
    "        tokenizer.fit_on_text(text)\n",
    "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def _load_word_vec(path, word2idx=None):\n",
    "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    word_vec = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split()\n",
    "        if word2idx is None or tokens[0] in word2idx.keys():\n",
    "            word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('loading embedding_matrix:', dat_fname)\n",
    "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        print('loading word vectors...')\n",
    "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
    "        fname = './glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
    "            if embed_dim != 300 else './glove.42B.300d.txt'\n",
    "        word_vec = _load_word_vec(fname, word2idx=word2idx)\n",
    "        print('building embedding_matrix:', dat_fname)\n",
    "        for word, i in word2idx.items():\n",
    "            vec = word_vec.get(word)\n",
    "            if vec is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = vec\n",
    "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "\n",
    "def pad(a,maxlen):\n",
    "    B = np.pad(a, (0, maxlen - len(a)%maxlen), 'constant')\n",
    "    return B\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, max_seq_len, lower=True):\n",
    "        self.lower = lower\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "\n",
    "    def fit_on_text(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        unknownidx = len(self.word2idx)+1\n",
    "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "\n",
    "class Tokenizer4Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "    \n",
    "    def add_tokens(self,params):\n",
    "        self.tokenizer.add_tokens(params)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from pytorch_transformers import BertModel,BertForTokenClassification,BertConfig\n",
    "# from transformers import BertModel,BertForTokenClassification,BertConfig\n",
    "\n",
    "# from models.knowledge_bert import BertForTokenClassification\n",
    "\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, Tokenizer4Bert, ABSADataset\n",
    "\n",
    "from models import LSTM, IAN, MemNet, RAM, TD_LSTM, Cabasc, ATAE_LSTM, TNet_LF, AOA, MGAN, LCF_BERT\n",
    "from models.aen import CrossEntropyLoss_LSR, AEN_BERT\n",
    "from models.bert_spc import BERT_SPC\n",
    "from models.bert_raw import BERT_RAW\n",
    "from models.bert_label import BERT_LABEL\n",
    "from models.bert_aspect import BERT_ASPECT\n",
    "from models.bert_target import BERT_TARGET\n",
    "from models.bert_multi_target import BERT_MULTI_TARGET\n",
    "from models.bert_kg import BERT_KG\n",
    "from models.bert_compete import BERT_COMPETE\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', default='bert_spc', type=str)\n",
    "parser.add_argument('--dataset', default='laptop', type=str, help='twitter, restaurant, laptop')\n",
    "parser.add_argument('--optimizer', default='adam', type=str)\n",
    "parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
    "parser.add_argument('--learning_rate', default=2e-5, type=float, help='try 5e-5, 2e-5 for BERT, 1e-3 for others')\n",
    "parser.add_argument('--dropout', default=0.1, type=float)\n",
    "parser.add_argument('--l2reg', default=0.01, type=float)\n",
    "parser.add_argument('--num_epoch', default=10, type=int, help='try larger number for non-BERT models')\n",
    "parser.add_argument('--batch_size', default=16, type=int, help='try 16, 32, 64 for BERT models')\n",
    "parser.add_argument('--log_step', default=5, type=int)\n",
    "parser.add_argument('--embed_dim', default=300, type=int)\n",
    "parser.add_argument('--hidden_dim', default=300, type=int)\n",
    "parser.add_argument('--bert_dim', default=768, type=int)\n",
    "parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)\n",
    "parser.add_argument('--max_seq_len', default=128, type=int)\n",
    "parser.add_argument('--polarities_dim', default=3, type=int)\n",
    "# parser.add_argument('--hops', default=3, type=int)\n",
    "parser.add_argument('--device', default=None, type=str, help='e.g. cuda:0')\n",
    "parser.add_argument('--seed', default=None, type=int, help='set seed for reproducibility')\n",
    "parser.add_argument('--valset_ratio', default=0, type=float, help='set ratio between 0 and 1 for validation support')\n",
    "parser.add_argument('--load_mode', default=0, type=int, help='load existed model')\n",
    "\n",
    "# The following parameters are only valid for the lcf-bert model\n",
    "parser.add_argument('--local_context_focus', default='cdm', type=str, help='local context focus mode, cdw or cdm')\n",
    "parser.add_argument('--SRD', default=3, type=int, help='semantic-relative-distance, see the paper of LCF-BERT model')\n",
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xiangpan/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/xiangpan/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/xiangpan/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "Adding [aspect_b] to the vocabulary\n",
      "Adding [aspect_e] to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30524, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.pretrained_bert_name)\n",
    "config = BertConfig.from_pretrained(opt.pretrained_bert_name, output_attentions=True)\n",
    "bert = BertModel.from_pretrained(opt.pretrained_bert_name,config=config)\n",
    "num_added_tokens = tokenizer.add_tokens(['[aspect_b]','[aspect_e]'])\n",
    "bert.resize_token_embeddings(len(tokenizer.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('./datasets/semeval14/Restaurants_Train.xml.seg', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "lines = fin.readlines()\n",
    "fin.close()\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for i in range(0, len(lines), 3):\n",
    "    text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "    aspect = lines[i + 1].lower().strip()\n",
    "    polarity = lines[i + 2].strip()\n",
    "\n",
    "    text_raw=\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\"\n",
    "    text_spc='[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\"\n",
    "    text_target='[CLS] ' + text_left + ' [aspect_b] '+aspect + ' [aspect_e] '+ text_right + ' [SEP] '\n",
    "\n",
    "    text_without_cls=text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\"\n",
    "\n",
    "\n",
    "    text_raw_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
    "\n",
    "\n",
    "    text_target_indics = tokenizer.text_to_sequence(text_target)\n",
    "    text_target_segments_ids=np.asarray([0] * (np.sum(text_target_indics != 0)))\n",
    "    text_target_segments_ids = pad_and_truncate(text_target_segments_ids, tokenizer.max_seq_len)\n",
    "\n",
    "\n",
    "    text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
    "    text_left_indices = tokenizer.text_to_sequence(text_left)\n",
    "    text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
    "    text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
    "    text_right_with_aspect_indices = tokenizer.text_to_sequence(\" \" + aspect + \" \" + text_right, reverse=True)\n",
    "    aspect_indices = tokenizer.text_to_sequence(aspect)\n",
    "    left_context_len = np.sum(text_left_indices != 0)\n",
    "    aspect_len = np.sum(aspect_indices != 0)\n",
    "    aspect_pos = left_context_len+1\n",
    "    target_begin=left_context_len+1\n",
    "    aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])\n",
    "    # aspect_range = torch.LongTensor(range(left_context_len.item()+1, (left_context_len + aspect_len).item()+1))# plus [cls]\n",
    "    polarity = int(polarity) + 1\n",
    "\n",
    "    text_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\")\n",
    "    # text_bert_indices = tokenizer.text_to_sequence('[CLS] '+ text_left + \" \" + aspect + \" \" + text_right + ' [SEP] '+ aspect + \" [SEP] \")\n",
    "\n",
    "    bert_segments_ids = np.asarray([0] * (np.sum(text_raw_indices != 0)+2) + [1] * (aspect_len + 1))\n",
    "    # bert_segments_ids = np.asarray([1] * (aspect_len + 1)+[0] * (np.sum(text_raw_indices != 0) + 2))\n",
    "    bert_raw_segments_ids=np.asarray([0] * (np.sum(text_raw_indices != 0)+2))\n",
    "    bert_segments_ids = pad_and_truncate(bert_segments_ids, tokenizer.max_seq_len)\n",
    "    bert_raw_segments_ids = pad_and_truncate(bert_raw_segments_ids, tokenizer.max_seq_len)\n",
    "    text_raw_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\")\n",
    "    aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
    "    input_mask=torch.tensor([1]*len(text_bert_indices))\n",
    "    # print(aspect_indices)\n",
    "    data = {\n",
    "        'text_target_indics':text_target_indics,\n",
    "        'text_target_segments_ids':text_target_segments_ids,\n",
    "        'aspect_pos':aspect_pos,\n",
    "        'aspect_len':aspect_len,\n",
    "        'target_begin':target_begin,\n",
    "        'text_raw': text_raw,\n",
    "        'text_spc': text_spc,\n",
    "        'text_without_cls': text_without_cls,\n",
    "        'text_aspect':aspect,\n",
    "        'left_context_len': left_context_len,\n",
    "        'text_bert_indices': text_bert_indices,\n",
    "        'bert_segments_ids': bert_segments_ids,\n",
    "        'text_raw_bert_indices': text_raw_bert_indices,\n",
    "        'aspect_bert_indices': aspect_bert_indices,\n",
    "        'text_raw_indices': text_raw_indices,\n",
    "        'bert_raw_segments_ids':bert_raw_segments_ids,\n",
    "        'text_raw_without_aspect_indices': text_raw_without_aspect_indices,\n",
    "        'text_left_indices': text_left_indices,\n",
    "        'text_left_with_aspect_indices': text_left_with_aspect_indices,\n",
    "        'text_right_indices': text_right_indices,\n",
    "        'text_right_with_aspect_indices': text_right_with_aspect_indices,\n",
    "        'aspect_indices': aspect_indices,\n",
    "        'aspect_in_text': aspect_in_text,\n",
    "        'polarity': polarity,\n",
    "        # 'polaritys':polaritys,\n",
    "        'input_mask':input_mask,\n",
    "    }\n",
    "\n",
    "    all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_token=tokenizer.text_to_sequence('[aspect_b]')[0]\n",
    "end_token=tokenizer.text_to_sequence('[aspect_e]')[0]\n",
    "\n",
    "idx=0       \n",
    "while idx in range(len(all_data)):\n",
    "    data=all_data[idx]\n",
    "    text_raw=data['text_raw']\n",
    "    flag = True\n",
    "    count=0\n",
    "    while flag:\n",
    "        count=count+1\n",
    "        if idx+count not in range(len(all_data)):\n",
    "            break\n",
    "        text_raw_next=all_data[idx+count]['text_raw']\n",
    "        if (text_raw_next!=text_raw):\n",
    "            flag=False\n",
    "    aspect_list=[]\n",
    "    for i in range(0,count):\n",
    "        text_aspect=all_data[idx+i]['text_aspect']\n",
    "        aspect_list.append(text_aspect)\n",
    "    for i in range(0,count):\n",
    "        all_data[idx+i]['aspect_list']=aspect_list\n",
    "    idx=idx+count\n",
    "a=np.array(all_data)\n",
    "np.save(\"all_data.npy\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding [target_b] to the vocabulary\n",
      "Adding [target_e] to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_tokens = tokenizer.add_tokens(['[aspect_b]','[aspect_e]'])\n",
    "num_added_tokens = tokenizer.add_tokens(['[target_b]','[target_e]'])\n",
    "bert.resize_token_embeddings(len(tokenizer.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_b=tokenizer.text_to_sequence('[target_b]')[0]\n",
    "target_e=tokenizer.text_to_sequence('[target_e]')[0]\n",
    "aspect_b=tokenizer.text_to_sequence('[aspect_b]')[0]\n",
    "aspect_e=tokenizer.text_to_sequence('[aspect_e]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30524"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=np.load('all_data.npy',allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_data)):\n",
    "    all_data[i]['text_multi']=all_data[i]['text_raw']\n",
    "    for aspect in all_data[i]['aspect_list']:\n",
    "        aspect_len=len(aspect)\n",
    "        text_multi=all_data[i]['text_multi']\n",
    "        if aspect == all_data[i]['text_aspect']:\n",
    "            text_multi=insert(text_multi,' [target_b] ',text_multi.find(aspect))\n",
    "            text_multi=insert(text_multi,' [target_e] ',text_multi.find(aspect)+len(aspect))\n",
    "        else:\n",
    "            text_multi=insert(text_multi,' [aspect_b] ',text_multi.find(aspect))\n",
    "            text_multi=insert(text_multi,' [aspect_e] ',text_multi.find(aspect)+len(aspect))\n",
    "        all_data[i]['text_multi']=text_multi\n",
    "    multi_target_indics = tokenizer.text_to_sequence(all_data[i]['text_multi'])\n",
    "    all_data[i]['multi_target_indics']=multi_target_indics\n",
    "    multi_target_segments_ids=np.asarray([0] * (np.sum(multi_target_indics != 0)))\n",
    "    multi_target_segments_ids = pad_and_truncate(multi_target_segments_ids, tokenizer.max_seq_len)\n",
    "    all_data[i]['multi_target_segments_ids']=multi_target_segments_ids\n",
    "    pos=np.argwhere(all_data[i]['multi_target_indics']==target_b)[0][0]\n",
    "#     print(pos,all_data[i]['multi_target_indics'])\n",
    "    all_data[i]['target_pos']=pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['staff']\n",
      "1 ['food']\n",
      "3 ['food', 'kitchen', 'menu']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-11f21506acb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mtext_multi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_multi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' [target_e] '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_multi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0maspect_b_now\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0maspect_e_now\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mtext_multi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_multi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maspect_b_now\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_multi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in range(len(all_data)):\n",
    "            all_data[i]['text_multi']=all_data[i]['text_raw']\n",
    "            now=0\n",
    "            print(len(all_data[i]['aspect_list']),all_data[i]['aspect_list'])\n",
    "            for aspect in all_data[i]['aspect_list']:\n",
    "                aspect_len=len(aspect)\n",
    "                text_multi=all_data[i]['text_multi']\n",
    "                if aspect == all_data[i]['text_aspect']:\n",
    "                    text_multi=insert(text_multi,' [target_b] ',text_multi.find(aspect))\n",
    "                    text_multi=insert(text_multi,' [target_e] ',text_multi.find(aspect)+len(aspect))\n",
    "                else:\n",
    "                    aspect_b_now=aspect_b[0]\n",
    "                    aspect_e_now=aspect_e[0]\n",
    "                    text_multi=insert(text_multi,' '+aspect_b_now+' ',text_multi.find(aspect))\n",
    "                    text_multi=insert(text_multi,' '+aspect_e_now+' ',text_multi.find(aspect)+len(aspect))\n",
    "                    now=now+1\n",
    "                    # text_multi=insert(text_multi,' [aspect_b] ',text_multi.find(aspect))\n",
    "                    # text_multi=insert(text_multi,' [aspect_e] ',text_multi.find(aspect)+len(aspect))\n",
    "                all_data[i]['text_multi']=text_multi\n",
    "            multi_target_indices = tokenizer.text_to_sequence(all_data[i]['text_multi'])\n",
    "            all_data[i]['multi_target_indices']=multi_target_indices\n",
    "            multi_target_segments_ids=np.asarray([0] * (np.sum(multi_target_indices != 0)))\n",
    "            multi_target_segments_ids = pad_and_truncate(multi_target_segments_ids, tokenizer.max_seq_len)\n",
    "            all_data[i]['multi_target_segments_ids']=multi_target_segments_ids\n",
    "            pos=np.argwhere(all_data[i]['multi_target_indices']==target_b)[0][0]\n",
    "            aspect_poss=np.argwhere(all_data[i]['multi_target_indices']==target_b)[0][0]\n",
    "            # print(pos,all_data[i]['multi_target_indices'])\n",
    "            all_data[i]['target_pos']=pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-714c2147a1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "text_raw.insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='[target_'+str(i)+'b]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[target_1b]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
